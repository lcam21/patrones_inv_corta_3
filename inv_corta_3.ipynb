{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "El procesamiento del lenguaje natural (NLP, por sus siglas en inglés) es el enfoque computarizado para analizar texto que se basa en un conjunto de teorías y un conjunto de tecnologías. NLP comenzó en la década de 1950 como la intersección de la inteligencia artificial y la lingüística. NLP originalmente era distinta de la recuperación de información de texto (IR), que emplea técnicas basadas en estadísticas altamente escalables para indexar y buscar grandes volúmenes de texto de manera eficiente. Actualmente, la NLP toma prestados de varios campos muy diversos, lo que requiere que los investigadores y desarrolladores de la NPL de hoy amplíen significativamente su base de conocimiento mental. Y, al ser un área muy activa de investigación y desarrollo, no hay una sola definición acordada que satisfaga a todos, pero hay algunos aspectos que formarían parte de la definición de cualquier persona con conocimiento [1], [2].\n",
    "\n",
    "__Definición:__ Procesamiento de lenguaje natural es una gama de técnicas computacionales motivadas teóricamente para analizar y representar textos que ocurren naturalmente en uno o más niveles de análisis lingüístico con el fin de lograr un procesamiento de lenguaje similar al humano para una variedad de tareas o aplicaciones. [2]\n",
    "\n",
    "El objetivo de la NLP como se indicó anteriormente es \"lograr un procesamiento de lenguaje similar al humano\". A pesar de que el campo de la NLP se denominó originalmente Entendimiento del Lenguaje Natural (NLU, por sus siglas en inglés) en los primeros días de la inteligencia artificial (AI, por sus siglas en ingles), hoy en día se está de acuerdo en que si bien la meta de la NLP es la NLU verdadera, esa meta aún no se ha logrado. Un sistema completo de NLU podría [2]:\n",
    "1. Parafrasear un texto de entrada\n",
    "2. Traducir el texto a otro idioma.\n",
    "3. Responder preguntas sobre el contenido del texto.\n",
    "4. Sacar inferencias del texto.\n",
    "\n",
    "Si bien la NLP ha hecho grandes avances en el logro de los objetivos 1 a 3, el hecho de que los sistemas de la NLP no puedan, por sí mismos, hacer inferencias a partir del texto, la NLU sigue siendo el objetivo de la NLP. [2]\n",
    "Si bien la comprensión semántica completa sigue siendo un objetivo muy lejano, los investigadores han adoptado un enfoque de dividir y conquistar e identificar varias tareas secundarias útiles para el desarrollo y análisis de aplicaciones. Estos van desde lo sintáctico, como el etiquetado de parte del discurso, fragmentación y análisis, hasta lo semántico, como la desambiguación del sentido de las palabras, el etiquetado de roles semánticos, la extracción de entidades denominadas y la resolución de la anáfora. [3]\n",
    "\n",
    "Actualmente, la mayoría de las investigaciones analizan esas tareas por separado. Muchos sistemas poseen pocas características que ayudarían a desarrollar una arquitectura unificada que presumiblemente sería necesaria para tareas semánticas más profundas. [3]\n",
    "\n",
    "En particular, muchos sistemas tienen tres fallos a este respecto: (i) son superficiales en el sentido de que el clasificador es a menudo lineal, (ii) para un buen desempeño con un clasificador lineal, deben incorporar muchas características específicas para la tarea; y (iii) las funciones en cascada aprendidas por separado de otras tareas, propagando así los errores. [3]\n",
    "\n",
    "### NLP paso a paso [8]\n",
    "\n",
    "Para este desarrollo se utilizará ejemplos que hacen referencia en el leguaje del inglés, por lo tanto se usaran muchas palabras o frases en inglés. Usando como referencia el siguiente texto de Wikipedia:\n",
    "London is the capital and most populous city of England and the United Kingdom. Standing on the River Thames in the south east of the island of Great Britain, London has been a major settlement for two millennia. It was founded by the Romans, who named it Londinium.\n",
    "\n",
    "Como se puede leer, el texto anterior contiene datos muy interesantes los cuales pueden ser de mucha ayuda si un algoritmo leyera y analizara este texto. Puedo entender que Londres es una ciudad localizada en Inglaterra. Pero para obtener esta información es necesario enseñar a nuestro algoritmo los conceptos básicos del lenguaje escrito.\n",
    "\n",
    "#### Paso 1: Segmentación del texto\n",
    "\n",
    "El primer paso es dividir el texto en oraciones. Asumiendo que cada idea esta separada en una oración obteniendo:\n",
    "\n",
    "1.\t“London is the capital and most populous city of England and the United Kingdom.”\n",
    "2.\t“Standing on the River Thames in the south east of the island of Great Britain, London has been a major settlement for two millennia.”\n",
    "3.\t“It was founded by the Romans, who named it Londinium.” \n",
    "\n",
    "Utiliza únicamente modelo de segmentación de texto puede ser simple solo usando los signos de puntuación. Pero NLP ofrece técnicas más complejas que trabajar mejor cuando los documentos no están escritos de forma clara.\n",
    "\n",
    "#### Paso 2: Tokenización de las palabras\n",
    "\n",
    "Una vez dividimos el texto en oraciones, se puede proceder a dividir esta oración en palabras. Tomando como ejemplo la primera oración: \n",
    "\n",
    "“_London is the capital and most populous city of England and the United Kingdom._”\n",
    "\n",
    "La oración se puede dividir en palabras o tokens. Esto es llamando tokenización. El cual tendría como resultado:\n",
    "\n",
    "“_London_”, “_is_”, “_the_”, _capital_”, “_and_”, “_most_”, “_populous_”, “_city_”, “_of_”, “_England_”, “_and_”, “_the_”, “_United_”, “_Kingdom_”, “_._”\n",
    "\n",
    "Tokenización es fácil, solamente es dividir las palabras por cada espacio que haya. Los signos de puntación se deberán tratar como tokes separados debido a que nos dan cierto sentido en las oraciones.\n",
    "\n",
    "#### Paso 3: Predecir partes del texto por cada token\n",
    "\n",
    "El siguiente paso es tomar cada palabra de la oración y tratar de descifrar que rol juega en el texto, por ejemplo, si es un sustantivo, verbo, adjetivo y así. Una vez conocido el rol de cada palabra en la oración nos ayudara a iniciar a entender sobre que trata la oración.\n",
    "\n",
    "Podemos hacer esto introduciendo cada palabra (y algunas palabras adicionales alrededor del contexto) en un modelo de clasificación pre-entrenado:\n",
    "\n",
    "<img src=\"img/picture1.png\">\n",
    "\n",
    "El modelo se entrenó originalmente al alimentarlo con millones de oraciones en inglés con el rol de cada palabra ya etiquetada y para hacer que aprendiera a replicar ese comportamiento. Se tiene que tomar en caso de que el modelo es completamente basado en estadísticas. Este no comprende que significa las palabras como lo hacen las personas. Este únicamente sabe cómo suponer una parte del texto basado en un oración similar y palabras que haya visto anterior. Después del procesado la oración puede verse así: \n",
    "\n",
    "<img src=\"img/picture2.png\">\n",
    "\n",
    "#### Paso 4: Lemmatización del texto\n",
    "\n",
    "En los idiomas una palabra puede aparecer en dos formas diferentes, como, por ejemplo:\n",
    "* I had a __pony__.\n",
    "* I had two __ponies__. \n",
    "\n",
    "Ambas oraciones hablan sobre el sustantivo pony, pero estos usan diferentes inflexiones. Por lo tanto, es importante que nuestro algoritmo conozca la base simple de cada palabra para identifique se la oración se esta refiriendo al mismo concepto. En la NLP se le llama a este proceso como lematización: determinar la forma más básica o el lema de cada palabra en la oración. \n",
    "\n",
    "La lematización generalmente se realiza al tener una tabla de consulta de las formas de lema de palabras en función de su parte en el texto y posiblemente tener algunas reglas personalizadas para manejar palabras que nunca ha visto antes.\n",
    "\n",
    "Así es como se ve la oración después de que la lematización, el único cambio se realiza en el verbo “be”, donde se cambia su forma de tercera persona por su forma base.\n",
    "\n",
    "<img src=\"img/picture3.png\">\n",
    "\n",
    "#### Paso 5: Identificación de stop words\n",
    "\n",
    "Palabras como: “_and_”, “_the_” y “_a_” introducen mucho ruido ya que aparecen mucho más frecuentemente que otras palabras. Algunas NLP los marcarán como stop words, es decir, palabras que tal vez desee filtrar antes de realizar un análisis estadístico. Así se vería nuestro ejemplo con las stop words en gris:\n",
    "\n",
    "<img src=\"img/picture4.png\">\n",
    "\n",
    "Las stop words generalmente se identifican simplemente al verificar una lista codificada de palabras de detención conocidas. Pero no hay una lista estándar de palabras de parada que sea apropiada para todas las aplicaciones. La lista de palabras a ignorar puede variar dependiendo de su aplicación.\n",
    "\n",
    "#### Paso 6: Análisis de dependencia\n",
    "\n",
    "El siguiente paso es analizar como todas las palabras de la oración están relacionadas con las otras. Como meta se tiene construir un árbol que asigne una única palabra padre a cada palabra en la oración. La raíz del árbol puede ser el verbo principal de la oración.\n",
    "\n",
    "<img src=\"img/picture5.png\">\n",
    "\n",
    "Pero podemos ir un paso más allá. Además de identificar la palabra principal de cada palabra, también podemos predecir el tipo de relación que existe entre esas dos palabras:\n",
    "\n",
    "<img src=\"img/picture6.png\">\n",
    "\n",
    "#### Paso 7: Reconocimiento de la entidad nombrada (NER)\n",
    "\n",
    "Ah este punto se puede iniciar a extraer ideas. En la oración, tenemos los siguientes sustantivos:\n",
    "\n",
    "<img src=\"img/picture7.png\">\n",
    "\n",
    "Algunos de estos nombres presentan cosas reales en el mundo. Por ejemplo, \"_London_\", \"_England_\" y \"_United Kingdom_\" representan lugares físicos en un mapa. Con esa información, podríamos extraer automáticamente una lista de los lugares del mundo real mencionados en un documento utilizando la NLP.\n",
    "\n",
    "El objetivo de NER, es detectar y etiquetar estos nombres con los conceptos del mundo real que representan. Así es como se ve nuestra oración después de ejecutar cada token a través del modelo de etiquetado NER:\n",
    "\n",
    "<img src=\"img/picture8.png\">\n",
    "\n",
    "Pero los sistemas NER no solo hacen una búsqueda simple en el diccionario. En su lugar, utilizan el contexto de cómo aparece una palabra en la oración y un modelo estadístico para adivinar qué tipo de sustantivo representa una palabra. Un buen sistema NER puede diferenciar entre \"_Brooklyn Decker_\" la persona y el lugar \"_Brooklyn_\" usando claves de contexto.\n",
    "\n",
    "Estos son solo algunos de los tipos de objetos que un sistema NER típico puede etiquetar:\n",
    "* Nombres de personas\n",
    "* Nombres de compania\n",
    "* Ubicaciones geográficas (tanto físicas como políticas)\n",
    "* Nombres de productos\n",
    "* Fechas y horarios\n",
    "* Cantidades de dinero\n",
    "* Nombres de eventos\n",
    "\n",
    "NER tiene muchos usos, ya que hace que sea tan fácil tomar datos estructurados del texto. Es una de las maneras más fáciles de obtener valor rápidamente de una tubería de NLP.\n",
    "\n",
    "### Paso 8: Resolución de la Coreferencia\n",
    "\n",
    "En este punto,se sabe las partes del comentario de cada palabra, cómo se relacionan entre sí y qué palabras se refieren a entidades nombradas.\n",
    "\n",
    "Sin embargo, el inglés está lleno de pronombres, palabras como _he_, _she_ y _it_. Estos son atajos que se usan en lugar de escribir nombres una y otra vez en cada oración. Los humanos pueden hacer un seguimiento de lo que representan estas palabras en función del contexto. Pero el modelo de NLP no sabe qué significan los pronombres porque solo examina una oración a la vez.\n",
    "\n",
    "Usando la tercera oración del texto:\n",
    "\"_It was founded by the Romans, who named it Londinium._\"\n",
    "\n",
    "Si se analiza, se entiende que \"_it_\" fue fundado por los romanos. Pero es mucho más útil saber que \"_London_\" fue fundada por los romanos.\n",
    "\n",
    "Como un ser humano que lee esta oración, puede darse cuenta fácilmente que \"_it_\" significa \"_London_\". El objetivo de la resolución de referencia es descubrir este mismo mapeo mediante el seguimiento de los pronombres en las oraciones. Se quiere averiguar todas las palabras que se refieren a la misma entidad.\n",
    "\n",
    "El resultado de ejecutar la resolución de referencia en el text para la palabra \"_London_\":\n",
    "\n",
    "<img src=\"img/picture9.png\">\n",
    "\n",
    "Con la información de referencia combinada con el árbol de análisis y la información de la entidad nombrada, se deberia poder extraer mucha información de este documento.\n",
    "\n",
    "La resolución de la referencia es uno de los pasos más difíciles de implementar. Es incluso más difícil que el análisis de oraciones. Los avances recientes en el aprendizaje profundo han dado como resultado nuevos enfoques que son más precisos, pero aún no son perfectos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "684830 exceeds max_map_len(32768)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-16609b30d635>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Load the large English NLP model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en_core_web_lg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\py3\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\py3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_link\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_package\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# installed as package\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_package\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# path to model data directory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\py3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model_from_package\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;34m\"\"\"Load a model from an installed package.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\py3\\lib\\site-packages\\en_core_web_lg\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(**overrides)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_init_py\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\py3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model_from_init_py\u001b[1;34m(init_file, **overrides)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE052\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath2str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\py3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model_from_path\u001b[1;34m(model_path, meta, **overrides)\u001b[0m\n\u001b[0;32m    154\u001b[0m             \u001b[0mcomponent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_pipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m             \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\py3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[1;34m(self, path, disable)\u001b[0m\n\u001b[0;32m    645\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;34m'vocab'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m             \u001b[0mexclude\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'vocab'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 647\u001b[1;33m         \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeserializers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    648\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\py3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[1;34m(path, readers, exclude)\u001b[0m\n\u001b[0;32m    509\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreader\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreaders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m             \u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    512\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\py3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(p)\u001b[0m\n\u001b[0;32m    633\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[1;34m'meta.json'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m             ('vocab', lambda p: (\n\u001b[1;32m--> 635\u001b[1;33m                 self.vocab.from_disk(p) and _fix_pretrained_vectors_name(self))),\n\u001b[0m\u001b[0;32m    636\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[1;34m'tokenizer'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m         ))\n",
      "\u001b[1;32mvocab.pyx\u001b[0m in \u001b[0;36mspacy.vocab.Vocab.from_disk\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mvectors.pyx\u001b[0m in \u001b[0;36mspacy.vectors.Vectors.from_disk\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\py3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[1;34m(path, readers, exclude)\u001b[0m\n\u001b[0;32m    509\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreader\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreaders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m             \u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    512\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mvectors.pyx\u001b[0m in \u001b[0;36mspacy.vectors.Vectors.from_disk.load_key2row\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mvectors.pyx\u001b[0m in \u001b[0;36mspacy.vectors.Vectors.from_disk.load_key2row\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\py3\\lib\\site-packages\\msgpack_numpy.py\u001b[0m in \u001b[0;36munpack\u001b[1;34m(stream, **kwargs)\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[0mobject_hook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'object_hook'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'object_hook'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_unpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0munpackb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpacked\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\py3\\lib\\site-packages\\msgpack\\__init__.py\u001b[0m in \u001b[0;36munpack\u001b[1;34m(stream, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \"\"\"\n\u001b[0;32m     56\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0munpackb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\py3\\lib\\site-packages\\msgpack_numpy.py\u001b[0m in \u001b[0;36munpackb\u001b[1;34m(packed, **kwargs)\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[0mobject_hook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'object_hook'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'object_hook'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 184\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_unpackb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpacked\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[0mload\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munpack\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mmsgpack/_unpacker.pyx\u001b[0m in \u001b[0;36mmsgpack._cmsgpack.unpackb\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: 684830 exceeds max_map_len(32768)"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the large English NLP model\n",
    "nlp = spacy.load('en_core_web_lg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estado actual\n",
    "\n",
    "Existen muchos esfuerzos alrededor de NLP, tanto a nivel de investigación como a nivel de aplicación. Algunos ejemplos de investigaciones son las presentadas en [3] y [4] donde se implementa una red neuronal profunda con aprendizaje multitarea para generar una gran cantidad de predicciones de procesamiento del lenguaje: etiquetas de parte del discurso, fragmentos, etiquetas de entidad nombradas, roles semánticos, palabras semánticamente similares y la probabilidad de que la oración tenga sentido (gramatical y semánticamente) utilizando un modelo de lenguaje. Investigaciones como [4] describimos núcleos para varias estructuras de lenguaje natural, permitiendo representaciones ricas y de alta dimensión de estas estructuras, mostrando cómo se puede aplicar un núcleo sobre árboles para analizar utilizando el algoritmo vottron perceptrón y dando resultados experimentales en el cuerpo ATIS de árboles parse. \n",
    "\n",
    "También se dan otros esfuerzos que involucra la medicina donde se toma la información clínica y se procesa por medio de NLP, resolviendo la falta de estructuración en la información, haciendo que los datos clínicos estén disponibles para su uso, caso presentado en [5]. Otra aplicación es la detección de informes de defectos duplicados mediante el procesamiento de lenguaje natural [6]. Una aplicación muy particular es la expuesta en [7], donde se presenta un analizador de opciones el cual extrae opiniones sobre un tema de documentos de texto en línea dando como resultado una extracción de términos de características específicas del tema, extracción de sentimiento y asociación (sujeto, sentimiento) por análisis de relación.\n",
    "\n",
    "Se pueden encontrar en el marcado aplicaciones muy famosas que utilizan este algoritmo como por ejemplo Cortana, Siri en la detección de comandos por vos e inclusive Gmail para la detección de spam\n",
    "\n",
    "### Referencias\n",
    "\n",
    "[1] Nadkarni, P. M., Ohno-Machado, L., & Chapman, W. W. (2011). Natural language processing: an introduction. Journal of the American Medical Informatics Association, 18(5), 544-551. \n",
    "\n",
    "[2] Liddy, E.D. (2001). Natural Language Processing. In Encyclopedia of Library and Information Science, 2nd Ed. NY. Marcel Decker, Inc.\n",
    "\n",
    "[3] Collobert, R., & Weston, J. (2008, July). A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning (pp. 160-167). ACM.\n",
    "\n",
    "[4] Collins, M., & Duffy, N. (2002). Convolution kernels for natural language. In Advances in neural information processing systems (pp. 625-632).\n",
    "\n",
    "[5] Friedman, C., & Hripcsak, G. (1999). Natural language processing and its future in medicine. Acad Med, 74(8), 890-5.\n",
    "\n",
    "[6] Runeson, P., Alexandersson, M., & Nyholm, O. (2007, May). Detection of duplicate defect reports using natural language processing. In Proceedings of the 29th international conference on Software Engineering (pp. 499-510). IEEE Computer Society.\n",
    "\n",
    "[7] Yi, J., Nasukawa, T., Bunescu, R., & Niblack, W. (2003, November). Sentiment analyzer: Extracting sentiments about a given topic using natural language processing techniques. In Third IEEE international conference on data mining (pp. 427-434). IEEE.\n",
    "\n",
    "[8] Geitgey,A. (2018). Natural Language Processing is Fun! In https://medium.com/@ageitgey/natural-language-processing-is-fun-9a0bff37854e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
